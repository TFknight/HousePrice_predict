# -*- coding: UTF-8 -*-
import numpy as np
import pandas as pd
import datetime
from sklearn.cross_validation import KFold
from sklearn.cross_validation import train_test_split
import time
from sklearn import preprocessing
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor
from sklearn.grid_search import GridSearchCV
from sklearn.cross_validation import ShuffleSplit
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.linear_model import Ridge, LassoCV, LassoLarsCV, ElasticNet
from sklearn.kernel_ridge import KernelRidge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn import linear_model,svm
from sklearn.ensemble import BaggingRegressor
from scipy.stats import skew


def create_submission(prediction, score):
    now = datetime.datetime.now()
    print score
    sub_file = 'submission_' + 'baobao4_linear' + '.csv'
    # sub_file = 'prediction_training.csv'
    print ('Creating submission: ', sub_file)
    pd.DataFrame({'Id': test['Id'].values, 'SalePrice': prediction}).to_csv(sub_file, index=False)


# train need to be test when do test prediction
def data_preprocess(train, test):

    outlier_idx = [4, 11, 13, 20, 46, 66, 70, 167, 178, 185, 199, 224, 261, 309, 313, 318, 349, 412, 423, 440, 454, 477,
                   478, 523, 540, 581, 588, 595, 654, 688, 691, 774, 798, 875, 898, 926, 970, 987, 1027, 1109, 1169,
                   1182, 1239, 1256, 1298, 1324, 1353, 1359, 1405, 1442, 1447]
    train.drop(train.index[outlier_idx], inplace=True)
    all_data = pd.concat((train.loc[:, 'MSSubClass':'SaleCondition'],
                          test.loc[:, 'MSSubClass':'SaleCondition']))

    to_delete = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']
    all_data = all_data.drop(to_delete, axis=1)

    train["SalePrice"] = np.log1p(train["SalePrice"])

    # log transform skewed numeric features

    numeric_feats = all_data.dtypes[all_data.dtypes != "object"].index
    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))  # compute skewness
    skewed_feats = skewed_feats[skewed_feats > 0.75]
    skewed_feats = skewed_feats.index
    all_data[skewed_feats] = np.log1p(all_data[skewed_feats])
    #数值转化，把不是数值的特征用one-hot编码来表示，
    # 这里那些不是数字的值就会变成新的一列，所以特征数会增加很多
    all_data = pd.get_dummies(all_data)
    all_data = all_data.fillna(method='ffill')

    X_train = all_data[:train.shape[0]]
    X_test = all_data[train.shape[0]:]
    y = train.SalePrice

    return X_train, X_test, y


def mean_squared_error_(ground_truth, predictions):
    return mean_squared_error(ground_truth, predictions) ** 0.5


RMSE = make_scorer(mean_squared_error_, greater_is_better=False)


class ensemble(object):
    def __init__(self, n_folds, stacker, base_models):
        self.n_folds = n_folds
        self.stacker = stacker
        self.base_models = base_models

    def fit_predict(self, train, test, ytr):

        X = train.values
        y = ytr.values
        T = test.values
        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=0))

        S_train = np.zeros((X.shape[0], len(self.base_models)))
        S_test = np.zeros((T.shape[0], len(self.base_models)))



        for i, reg in enumerate(base_models):
            print ("Fitting the base model...")
            S_test_i = np.zeros((T.shape[0], len(folds)))
            for j, (train_idx, test_idx) in enumerate(folds):

                X_train = X[train_idx]
                y_train = y[train_idx]
                X_holdout = X[test_idx]
                reg.fit(X_train, y_train)
                #pred list
                y_pred = reg.predict(X_holdout)

                S_train[test_idx, i] = y_pred
                S_test_i[:, j] = reg.predict(T)
            S_test[:, i] = S_test_i.mean(1)

        print ("Stacking base models...")
        # tuning the stacker
        param_grid = {
            # 'alpha': [ 0.2, 0.3, 0.4, 0.5, 0.8, 1e0, 3, 5, 7, 1e1],
        }
        grid = GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, scoring=RMSE)
        grid.fit(S_train, y)
        try:
            print('Param grid:')
            print(param_grid)
            print('Best Params:')
            print(grid.best_params_)
            print('Best CV Score:')
            print(-grid.best_score_)
            print('Best estimator:')
            print(grid.best_estimator_)

        except:
            pass

        y_pred = grid.predict(S_test)


        return y_pred, -grid.best_score_


train = pd.read_csv("../../input/train.csv")  # read train data
test = pd.read_csv("../../input/test.csv")  # read test data

# build a model library (can be improved)
base_models = [
    RandomForestRegressor(
        n_jobs=1, random_state=0,
        n_estimators=160, max_depth=11
    ),
    RandomForestRegressor(
        n_jobs=1, random_state=0,
        n_estimators=150, max_features=12,
        max_depth=11
    ),
    ExtraTreesRegressor(
        n_jobs=1, random_state=0,
        n_estimators=800, max_features=15
    ),
    ExtraTreesRegressor(
        n_jobs=1, random_state=0,
        n_estimators=800, max_features=15
    ),
    GradientBoostingRegressor(
        random_state=0,
        n_estimators=800, max_features=10, max_depth=15,
        learning_rate=0.01, subsample=1
    ),
    GradientBoostingRegressor(
        random_state=0,
        n_estimators=800, max_features=15, max_depth=15,
        learning_rate=0.01, subsample=1
    ),
    XGBRegressor(
        seed=0,
        n_estimators=800, max_depth=15,
        learning_rate=0.01, subsample=1, colsample_bytree=0.75
    ),

    XGBRegressor(
        seed=0,
        n_estimators=800, max_depth=12,
        learning_rate=0.01, subsample=0.8, colsample_bytree=0.75
    ),
    LassoCV(alphas=[1, 0.1, 0.001, 0.0005, 0.0002 ,0.0001, 0.00005]),
    KNeighborsRegressor(n_neighbors=5),
    KNeighborsRegressor(n_neighbors=10),
    KNeighborsRegressor(n_neighbors=15),
    KNeighborsRegressor(n_neighbors=25),
    LassoLarsCV(),
    ElasticNet(),
    SVR()
]

ensem = ensemble(
    n_folds=10,
    stacker=linear_model.LinearRegression(),
    base_models=base_models
)

X_train, X_test, y_train = data_preprocess(train,test)
y_pred, score = ensem.fit_predict(X_train, X_test, y_train)

create_submission(np.expm1(y_pred), score)

